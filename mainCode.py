# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fEmhcbgUckzRW16g-vUC6dklzFggTK9j
"""

# Nouran Ahmed 20200609
# Mariam Tarek 20200523
# Nada Ashraf  20200587
# Fatma Salah  20200376
# Farah Tawfiq 20200378

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn import tree
import statistics
from collections import Counter
from matplotlib.backends.backend_pdf import PdfPages
from nltk.corpus import stopwords
import nltk
from nltk.tokenize import word_tokenize
import spacy
import string
import re
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, classification_report
from sklearn.neural_network import MLPClassifier
from keras.optimizers import Adam
import joblib
import pickle
from nltk.tokenize import word_tokenize
from google.colab import drive


import tensorflow.keras as keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense


import warnings
warnings.filterwarnings("ignore")

# Load the "drug.csv" dataset
data = pd.read_csv("./sentimentdataset (Project 1).csv")
# Print information about the old dataset
print(data.head())

# Check for missing values in the dataset
missing_values = data.isnull().sum()
# Display the count of missing values for each column
print("\nMissing Values:")
print(missing_values)
# Check if there are any missing values in the entire dataset
if missing_values.sum() == 0:
    print("\nNo missing values in the dataset.")
else:
    print("\nThere are missing values in the dataset.")

# Examining the distribution of samples in each class.

# info about the dataset
print("Info about the dataset: ")
print(data.info())
print("------------------------------------------")
# count the num of samples in each class
class_distribution = data['Target'].value_counts()
print(class_distribution)
print("-------------------------------------------")
class_distribution.plot(kind='bar', color=['green', 'red'])
plt.title('Class Distribution')
plt.xlabel('Class')
plt.ylabel('Count')
plt.show()

# separating targets and features
target_column = ['Target', 'ID', 'Source']

X = data.drop(target_column, axis=1)
y = pd.DataFrame(data['Target'], columns=['Target'])

print(X.head()) #features
print(y.head()) #target

nltk.download('stopwords')
stopWords = set(stopwords.words('english'))
negationWords = set(["hadn't", "wouldn't", "doesn't", "mightn't", "won't", "shouldn't", 'haven', 'aren' , 'doesn', 'couldn', 'didn', 'isn', 'wouldn', 'mustn', "isn't", "shan't", "didn't", 'shan', 'hadn', 'wasn', 'weren', "hasn't", 'mightn', "couldn't", "needn't", "haven't", "weren't", "aren't", 'needn', 'not', 'shouldn', 'hasn', "mustn't", "wasn't", "don't", 'don'])
stopWords = stopWords - negationWords
print(stopWords)

# remove punctuation
def removePunctuation(sentence):
    sentenceWithoutPunc = ""
    # iterates over each char and chick if it's punctuation char, not include it, then joins the other char whitout the punctuation
    sentenceWithoutPunc = "".join(i for i in sentence if i not in string.punctuation)
    return sentenceWithoutPunc

# apply the removePunctuation function on the dataframe
X['Message'] = X['Message'].apply(lambda sentence : removePunctuation(sentence))

# print(removePunctuation("Stopped by during don't the late May bank holiday off Rick Steve recommendation and loved it......"))

# lower casing the sentences
X['Message'] = X['Message'].apply(lambda char: char.lower())
print(X.head())

# Tokenization
def Tokenization(sentence):
    # split function is used to split the sentence into tokens
    tokens = re.split(r'\W+', sentence)
    return tokens

# apply the Tokenization function on the dataframe
X['Message'] = X['Message'].apply(lambda sentence: Tokenization(sentence))
print(X.head())
# print(Tokenization('Hello hell hhhh'))

# Remove stop words

# spacy.cli.download("en_core_web_sm")
nlp = spacy.load('en_core_web_sm')
default_stop_words = set(nlp.Defaults.stop_words)
negationWords = set(["hadn't", "wouldn't", "doesn't", "mightn't", "won't", "shouldn't", 'haven', 'aren' , 'doesn', 'couldn', 'didn', "didnt",'isn', 'wouldn', 'mustn', "isn't", "shan't", "didn't", 'shan', 'hadn', 'wasn', 'weren', "hasn't", 'mightn', "couldn't", "needn't", "haven't", "weren't", "aren't", 'needn', 'not', 'shouldn', 'hasn', "mustn't", "wasn't", "don't", 'don'])

custom_stop_words = default_stop_words - negationWords
nlp.Defaults.stop_words = custom_stop_words
# print(nlp)
X_filter =X
X_filter = pd.DataFrame(X)
#X_filter = pd.DataFrame(data)
def stopWordsRemoval(sentenceTokenized):
    allInfo = nlp(' '.join(sentenceTokenized))
    filtered_tokens = []

    for token in allInfo:
      # check if the lowercase of the text is not in the stop words
      if token.text.lower() not in map(str.lower, custom_stop_words):
        filtered_tokens.append(token.text)

    return filtered_tokens


# apply the stopWordsRemoval function on the dataframe
X_filter['Message'] = X_filter['Message'].apply(lambda sentence: stopWordsRemoval(sentence))
print(X_filter.head())
# print(stopWordsRemoval(["HI i not dokey do "]))

# Lemmatization using spaCy

X_filter = pd.DataFrame(X)
#print(X_filter)
def lemmatize(tokens):
    doc = nlp(' '.join(tokens))
    # Extract lemmatized tokens
    lemmatized_tokens = [token.lemma_ for token in doc]
    return lemmatized_tokens

# apply lemmatize function on the dataframe
X_filter['Message'] = X_filter['Message'].apply(lambda sentence: lemmatize(stopWordsRemoval(sentence)))
print(X_filter.head())
# print(lemmatize([['Now I am getting angry and I want my damn pho.']))

# Generate sentence embeddings
X = X_filter['Message'].apply(lambda sentence: ' '.join(sentence))

count_vectorizer = CountVectorizer()

X_vectorized = count_vectorizer.fit_transform(X)

# Save the CountVectorizer model to a file, we'll use it in the test file
joblib.dump(count_vectorizer, 'count_vectorizer.pkl')

# Convert X_vectorized to a DataFrame
X_df = pd.DataFrame(X_vectorized.toarray(), columns=count_vectorizer.get_feature_names_out())

print(X_df)

# Split into training and testing sets.
X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size = 0.2, random_state = 42)
# print(X_train)
# print("_________________________")
# print(X_test)
# print("_________________________")
# print(y_train)
# print("_________________________")
# print(y_test)
# print("_________________________")

# Initial Experiment:
svm = LinearSVC()

# Set up parameter grid for Grid Search
param_grid = {'C': [0.01, 0.1, 10, 100]}

# GridSearchCV to find the best params
grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)


print("Best Parameters:", grid_search.best_params_)

# predicting the targets of the x_test
y_pred_svc = grid_search.predict(X_test)

# Get the best model to use it to predict the answers in the test file
best_model = grid_search.best_estimator_

# Save the best model to a file, as we'll use it in the test file
joblib.dump(best_model, 'linear_svc_model.pkl')


# find the accuracy of the LinearSVC model
accuracy = accuracy_score(y_test, y_pred_svc)
print("Accuracy on Testing Set:", accuracy)

# Classification report of the LinearSVC model
print("______________________________________________")
print("Classification Report of The LinearSVC Model:")
print(classification_report(y_test, y_pred_svc))

model = Sequential()

# 10 is the number of neorens in the hidden layer, input_dim is the size of the input lauer(num of featueres)
model.add(Dense(10, input_dim=X_train.shape[1], activation='relu'))

# the output layer
model.add(Dense(1, activation='sigmoid'))

# set the new_learning_rate of the adam optimizer
new_learning_rate=0.0001
optimizer=Adam(learning_rate=new_learning_rate)

# compile the model
model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))
y_pred = model.predict(X_test)
y_pred_binary = np.round(y_pred)

# calculate the accuracy of the ANN model
accuracy = accuracy_score(y_test, y_pred_binary)
print("Accuracy:", accuracy)


# Classification report of the (ANN model
print("______________________________________________")
print("Classification Report of The (ANN Model:")
report = classification_report(y_test, y_pred_binary)
print(report)

# Mount Google Drive
drive.mount('/content/drive')